{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy\n",
    "====\n",
    "\n",
    "There's an excellent explanation of Cross-Entropy and related functions on https://machinelearningmastery.com/cross-entropy-for-machine-learning/ (Brownlee, 2019)\n",
    "\n",
    "Brownlee has some good explanation with code on cross-entropy from scratch, lets first look at how it's implemented in PyTorch and how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss (with torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming, batch first, we have \n",
    "# 5 data points, with 2 real number output each.\n",
    "# Each output representing perceptron's prediction for one label. \n",
    "last_layer = torch.randn(5, 2)\n",
    "predictions = torch.sigmoid(last_layer)\n",
    "\n",
    "# Correspondingly, we have 5 data points with 1 label each.\n",
    "# Each label has its corresponding integer to represent.\n",
    "truth = torch.LongTensor(5, 1).random_(0,2).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7833, -0.6955],\n",
       "        [-0.3951, -1.2426],\n",
       "        [ 0.5134, -1.4385],\n",
       "        [-0.5044,  0.4975],\n",
       "        [-1.4518, -1.5407]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3136, 0.3328],\n",
       "        [0.4025, 0.2240],\n",
       "        [0.6256, 0.1918],\n",
       "        [0.3765, 0.6219],\n",
       "        [0.1897, 0.1764]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each data point, we output the \n",
    "# sigmoidal output per label. \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each data point has a label and our label space\n",
    "# is made up of labels 0s and 1s.\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(predictions, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6149)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross-Entropy Loss (with torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Assuming, batch first, we have \n",
    "# 5 data points, with 3 real number output each.\n",
    "last_layer = torch.randn(5, 3)\n",
    "predictions = torch.sigmoid(last_layer)\n",
    "\n",
    "# Correspondingly, we have 5 data points, \n",
    "# with 3 boolean labels each.\n",
    "truth = torch.LongTensor(5, 3).random_(0,2).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0245,  0.2092,  0.5374],\n",
       "        [-0.6163,  1.1205,  0.4234],\n",
       "        [ 0.7822,  0.8506,  0.2875],\n",
       "        [ 1.0529,  1.7423,  0.5177],\n",
       "        [ 0.5219, -0.5256, -1.3008]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer # Before activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1167, 0.5521, 0.6312],\n",
       "        [0.3506, 0.7541, 0.6043],\n",
       "        [0.6861, 0.7007, 0.5714],\n",
       "        [0.7413, 0.8510, 0.6266],\n",
       "        [0.6276, 0.3715, 0.2140]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions # After activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is kind of special such that for each\n",
    "# data point we have 3 labels. And within\n",
    "# torch.autograd, it's design to compute any arbitrary label spaces. \n",
    "# Here, we're \"cheating\" the outputs by saying the space is 0s or 1s.\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(predictions, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5796)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens when the space isn't just 0s or 1s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Assuming, batch first, we have \n",
    "# 5 data points, with 3 real number output each.\n",
    "last_layer = torch.randn(5, 3)\n",
    "predictions = torch.sigmoid(last_layer)\n",
    "\n",
    "# Correspondingly, we have 5 data points, \n",
    "# with 3 boolean labels each.\n",
    "truth = torch.LongTensor(5, 3).random_(0,5).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3793, 0.7710, 0.5964],\n",
       "        [0.9048, 0.8805, 0.4903],\n",
       "        [0.2629, 0.6860, 0.6095],\n",
       "        [0.5221, 0.2022, 0.6634],\n",
       "        [0.1605, 0.3513, 0.6699]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 4., 0.],\n",
       "        [2., 2., 1.],\n",
       "        [2., 1., 2.],\n",
       "        [4., 4., 2.],\n",
       "        [2., 2., 1.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is kind of special such that for each\n",
    "# data point we have 3 labels. And within\n",
    "# torch.autograd, it's design to compute any arbitrary label spaces. \n",
    "# Here, we're \"cheating\" the outputs by saying the space is 0s or 1s.\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(predictions, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5911)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But how does that single scalar do backpropagation?\n",
    "\n",
    "We don't do backpropagation on that sum loss =)\n",
    "\n",
    "When we log the sum loss over all the data points, we get a scalar but because we have the loss for all labels in the label space, we actually get a vector back for every data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3136, 0.3328],\n",
       "        [0.4025, 0.2240],\n",
       "        [0.6256, 0.1918],\n",
       "        [0.3765, 0.6219],\n",
       "        [0.1897, 0.1764]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3136, 0.3328]) \t tensor([1, 0])\n",
      "[1.6729376316070557, 0.0]\n",
      "\n",
      "tensor([0.4025, 0.2240]) \t tensor([1, 0])\n",
      "[1.3129408359527588, 0.0]\n",
      "\n",
      "tensor([0.6256, 0.1918]) \t tensor([1, 0])\n",
      "[0.676690399646759, 0.0]\n",
      "\n",
      "tensor([0.3765, 0.6219]) \t tensor([0, 1])\n",
      "[0.0, 0.6853156685829163]\n",
      "\n",
      "tensor([0.1897, 0.1764]) \t tensor([1, 0])\n",
      "[2.3980143070220947, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we iterate through each data point. \n",
    "for row_pred, row_truth in zip(predictions, torch.nn.functional.one_hot(truth)):\n",
    "    row_entropy = [-1 * float(t * math.log2(p)) for p, t in zip(row_pred, row_truth)]\n",
    "    print(row_pred, '\\t', row_truth)\n",
    "    print(row_entropy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = xor_input = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = xor_output = np.array([0,1,1,0]).T\n",
    "\n",
    "X_pt = torch.tensor(X).float()\n",
    "Y_pt = torch.tensor(Y, requires_grad=False).squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 5\n",
    "num_data, input_dim = 4, 2\n",
    "num_data, output_dim = 4, 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                      nn.Sigmoid(), \n",
    "                      nn.Linear(hidden_dim, output_dim),\n",
    "                      nn.Sigmoid())\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4089, 0.5076],\n",
       "        [0.4006, 0.5109],\n",
       "        [0.3846, 0.5483],\n",
       "        [0.3770, 0.5506]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(X_pt)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth = Y_pt\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(predictions, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6954, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.6037, -0.0889],\n",
       "         [ 0.6657,  0.2893],\n",
       "         [-0.4809, -0.2565],\n",
       "         [-0.3140,  0.6397],\n",
       "         [ 0.4835,  0.2135]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.2218,  0.2330,  0.6987, -0.1540,  0.1504], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.2135, -0.2679,  0.2896, -0.0270,  0.0190],\n",
       "         [ 0.0608,  0.2798, -0.4467, -0.3066,  0.3231]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.3147,  0.1126], requires_grad=True)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6037, -0.0889],\n",
       "        [ 0.6657,  0.2893],\n",
       "        [-0.4809, -0.2565],\n",
       "        [-0.3140,  0.6397],\n",
       "        [ 0.4835,  0.2135]], requires_grad=True)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5187e-04,  2.5118e-04],\n",
       "        [ 8.1553e-05, -4.9206e-04],\n",
       "        [-9.7865e-04, -9.6398e-04],\n",
       "        [-4.5266e-04, -3.8973e-04],\n",
       "        [ 1.8965e-04, -2.3537e-05]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
