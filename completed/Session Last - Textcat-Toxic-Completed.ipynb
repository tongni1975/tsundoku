{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> table {float:left} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> table {float:left} </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/2.7/bin/pip\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "TypeError: 'module' object is not callable\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: No module named nltk\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tqdm lazyme nltk gensim\n",
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Toxic Comments\n",
    "\n",
    "Lets apply what we learnt in a realistic task and **fight cyber-abuse with NLP**!\n",
    "\n",
    "From https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/\n",
    "\n",
    "> *The threat of abuse and harassment online means that many people stop <br>*\n",
    "> *expressing themselves and give up on seeking different opinions. <br>*\n",
    "> *Platforms struggle to effectively facilitate conversations, leading many <br>*\n",
    "> *communities to limit or completely shut down user comments.*\n",
    "\n",
    "\n",
    "The goal of the task is to build a model to detect different types of of toxicity:\n",
    "\n",
    " - toxic\n",
    " - severe toxic\n",
    " - threats\n",
    " - obscenity\n",
    " - insults\n",
    " - identity-based hate\n",
    " \n",
    "In this part, you'll be munging the data as how I would be doing it at work. \n",
    "\n",
    "Your task is to train a feed-forward network on the toxic comments given the skills we have accomplished thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging into the data...\n",
    "\n",
    "If you're using linux/Mac you can use these bang commands in the notebook:\n",
    "\n",
    "```\n",
    "!pip3 install kaggle\n",
    "!mkdir -p /content/.kaggle/\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > /content/.kaggle/kaggle.json\n",
    "!chmod 600 /content/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "!unzip /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*\n",
    "```\n",
    "\n",
    "Otherwise, download the data from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "mkdir: /content/.kaggle/: Read-only file system\n",
      "/bin/sh: /content/.kaggle/kaggle.json: No such file or directory\n",
      "chmod: /content/.kaggle/kaggle.json: No such file or directory\n",
      "unzip:  cannot find or open /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*, /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*.zip or /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*.ZIP.\n",
      "\n",
      "No zipfiles found.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kaggle\n",
    "!mkdir -p /content/.kaggle/\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > /content/.kaggle/kaggle.json\n",
    "!chmod 600 /content/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "!unzip /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79        Hi! I am back again!\\nLast warning!\\nStop undo...\n",
       "176       I think that your a Fagget get a oife and burn...\n",
       "600       I'm also a sock puppet of this account...SUPRI...\n",
       "802       Fuck you, Smith. Please have me notified when ...\n",
       "1017      WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'L...\n",
       "                                ...                        \n",
       "157718    bitch \\nyou are a fucking hore. you suck dick ...\n",
       "158717    stupid head \\n\\nYOur dumb and you are stupid d...\n",
       "158856    Hey \\n\\nhey faggot, are you dead yet? or are y...\n",
       "159029                                  Death to Musulmans!\n",
       "159400    Shalom \\n\\nSemite, get the fuck out of here. I...\n",
       "Name: comment_text, Length: 478, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['threat'] == 1]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please stop. If you continue to ignore our policies by introducing inappropriate pages to Wikipedia, you will be blocked.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[3712]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['comment_text_tokenzied'] = df_train['comment_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"0000997932d777bf\",\"Explanation\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"0000997932d777bf\",\"Explanation\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"2. Aims: Mountolive seemed to insist in claiming (erroneously) that ETA's principal aim is to create a socialist state. This is not real, has been discussed before and can be documented. The current status is ok.\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"2. Aims: Mountolive seemed to insist in claiming (erroneously) that ETA's principal aim is to create a socialist state. This is not real, has been discussed before and can be documented. The current status is ok.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"0001b41b1c6bb37e\",\"\"\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"0001b41b1c6bb37e\",\"\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"3. Secondary tactical targets: \"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"3. Secondary tactical targets: \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"More\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"More\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Drug traffickers (this has a \"\"\"\"citation needed\"\"\"\" mark). I can't provide the citation but can say that in the early 80s it was the case. \"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Drug traffickers (this has a \"\"\"\"citation needed\"\"\"\" mark). I can't provide the citation but can say that in the early 80s it was the case. \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"6bdcf5533277818a\",\"\"\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"6bdcf5533277818a\",\"\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"\"\"types of accidents\"\"\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"\"\"types of accidents\"\"\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Rock or pop?\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Rock or pop?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"I would like a source for the Leitzaran highway, a more recent case, that I think is not so clear though. \"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"I would like a source for the Leitzaran highway, a more recent case, that I think is not so clear though. \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"I know it says rock and roll here, but the red album really does sound very teeny-popish. My music collection only has room for single genres (as in rock or pop, not rock/pop), so which one is it really?  \"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"I know it says rock and roll here, but the red album really does sound very teeny-popish. My music collection only has room for single genres (as in rock or pop, not rock/pop), so which one is it really?  \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\"\",0,0,0,0,0,0\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\"\",0,0,0,0,0,0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>251971 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "251971 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/liling.tan/git-stuff/tsundoku/completed/train.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/liling.tan/git-stuff/tsundoku/completed/train.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.13068 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.13068 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"0000997932d777bf\",\"Explanation\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"0000997932d777bf\",\"Explanation\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"0001b41b1c6bb37e\",\"\"\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"0001b41b1c6bb37e\",\"\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"More\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"More\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"\"\"types of accidents\"\"\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"\"\"types of accidents\"\"\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\"\",0,0,0,0,0,0\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\"\",0,0,0,0,0,0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"00025465d4725e87\",\"\"\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"00025465d4725e87\",\"\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"0005300084f90edc\",\"\"\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"0005300084f90edc\",\"\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Fair use rationale for Image:Wonju.jpg\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Fair use rationale for Image:Wonju.jpg\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Thanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair u...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Thanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair u...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Please go to the image description page and edit it to include a fair use rationale.\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Please go to the image description page and edit it to include a fair use rationale.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"6bdcf5533277818a\",\"\"\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"6bdcf5533277818a\",\"\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"2. Aims: Mountolive seemed to insist in claiming (erroneously) that ETA's principal aim is to create a socialist state. This is not real, has been discussed before and can be documented. The current status is ok.\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"2. Aims: Mountolive seemed to insist in claiming (erroneously) that ETA's principal aim is to create a socialist state. This is not real, has been discussed before and can be documented. The current status is ok.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 64740 lines. Lines per second: 62285.7</pre>"
      ],
      "text/plain": [
       "Read 64740 lines. Lines per second: 62285.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>329896 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "329896 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/liling.tan/git-stuff/tsundoku/completed/train.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/liling.tan/git-stuff/tsundoku/completed/train.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 85122 lines in 1.16068 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 85122 lines in 1.16068 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turicreate import SFrame\n",
    "sf_train = SFrame.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Just in case your Jupyter kernel dies, save the tokenized text =)\n",
    "\n",
    "# To save your tokenized text you can do this:\n",
    "import pickle\n",
    "with open('train_tokenized_text.pkl', 'wb') as fout:\n",
    "    pickle.dump(df_train['comment_text_tokenzied'], fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back:\n",
    "import pickle\n",
    "with open('train_tokenized_text.pkl', 'rb') as fin:\n",
    "    df_train['comment_text_tokenzied'] = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get a one-hot?\n",
    "\n",
    "There are many variants of how to get your one-hot embeddings from the individual columns.\n",
    "\n",
    "This is one way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_column_names = \"toxic\tsevere_toxic\tobscene\tthreat\tinsult\tidentity_hate\".split()\n",
    "df_train[label_column_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(df_train[label_column_names].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot to indices of the column.\n",
    "\n",
    "print(np.argmax(df_train[label_column_names].values, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.vocab = Dictionary(texts)\n",
    "        special_tokens = {'<pad>': 0, '<unk>':1}\n",
    "        self.vocab = Dictionary(texts)\n",
    "        self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Vectorize labels\n",
    "        self.labels = torch.tensor(labels)\n",
    "        # Keep track of how many data points.\n",
    "        self._len = len(texts)\n",
    "        \n",
    "        # Find the longest text in the data.\n",
    "        self.max_len = max(len(txt) for txt in texts)\n",
    "        \n",
    "        self.num_labels = len(labels[0])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vectorized_sent = self.vectorize(self.texts[index])\n",
    "        # To pad the sentence:\n",
    "        # Pad left = 0; Pad right = max_len - len of sent.\n",
    "        pad_dim = (0, self.max_len - len(vectorized_sent))\n",
    "        vectorized_sent_padded = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "        return {'x':vectorized_sent_padded, \n",
    "                'y':self.labels[index], \n",
    "                'x_len':len(vectorized_sent)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        # Lets just cast list of indices into torch tensors directly =)\n",
    "        return torch.tensor(self.vocab.doc2idx(tokens))\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m pip install -U pip\n",
    "#! python -m pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_column_names = \"toxic\tsevere_toxic\tobscene\tthreat\tinsult\tidentity_hate\".split()\n",
    "toxic_data = ToxicDataset(df_train['comment_text_tokenzied'],\n",
    "                          df_train[label_column_names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Should', 'say', 'something', 'about', 'his', 'views', 'as', 'an', 'educationalist', 'and', 'socialist', 'political', 'commentator', '.', 'Link', 'to', 'http', ':', '//www.langandlit.ualberta.ca/Fall2004/SteigelBainbridge.html', 'mentions', 'this', 'a', 'bit', '-', 'he', 'stood', 'as', 'an', 'election', 'candidate', 'for', 'Respect', '.']\n"
     ]
    }
   ],
   "source": [
    "print(toxic_data.texts[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "dataloader = DataLoader(dataset=toxic_data, \n",
    "                        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[ 6, 15, 38,  ...,  0,  0,  0],\n",
       "         [50, 44, 51,  ...,  0,  0,  0]]), 'y': tensor([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]), 'x_len': tensor([49, 27])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNet(nn.Module):\n",
    "    def __init__(self, max_len, num_labels, vocab_size, embedding_size, hidden_dim):\n",
    "        super(FFNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                       embedding_dim=embedding_size, \n",
    "                                       padding_idx=0)\n",
    "        # The no. of inputs to the linear layer is the \n",
    "        # no. of tokens in each input * embedding_size\n",
    "        self.linear1 = nn.Linear(embedding_size*max_len, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # We want to flatten the inputs so that we get the matrix of shape.\n",
    "        # batch_size x no. of tokens in each input * embedding_size\n",
    "        batch_size, max_len = inputs.shape\n",
    "        embedded = self.embeddings(inputs).view(batch_size, -1)\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        return torch.sigmoid(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697128176689148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:04<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43457943201065063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3127411901950836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2765364944934845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:04<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18910646438598633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:05<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21500389277935028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26081159710884094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16950567066669464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2470753788948059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:04<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21235232055187225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:05<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22905446588993073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:08<?, ?it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18457254767417908\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# Hint: the CBOW model object you've created.\n",
    "model = FFNet(toxic_data.max_len, \n",
    "              len(label_column_names),\n",
    "              toxic_data.vocab_size, \n",
    "              embedding_size=embedding_size, \n",
    "              hidden_dim=hidden_size).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#model = nn.DataParallel(model)\n",
    "\n",
    "training_losses = []\n",
    "num_epochs = 100\n",
    "for _e in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        # Zero gradient.\n",
    "        optimizer.zero_grad()\n",
    "        # Feed forward.\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(float(loss))\n",
    "        break\n",
    "    print(sum(epoch_loss)/len(epoch_loss))\n",
    "    training_losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(text):\n",
    "    # Vectorize and Pad.\n",
    "    vectorized_sent = toxic_data.vectorize(word_tokenize(text))\n",
    "    pad_dim = (0, toxic_data.max_len - len(vectorized_sent))\n",
    "    vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "    # Forward Propagation.\n",
    "    # Unsqueeze because model is expecting `batch_size` x `sequence_len` shape.\n",
    "    outputs = model(vectorized_sent.unsqueeze(0)).squeeze()\n",
    "    # To get the boolean output, we check if outputs are > 0.5\n",
    "    return [int(l > 0.5) for l in outputs]\n",
    "    # What happens if you use torch.max instead? =)\n",
    "    ##return label_column_names[int(torch.max(outputs, dim=1).indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a nice message.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(label_column_names)\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do \"quick and dirty\" with Simple Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/db/801999c78cf949652d16096fd2d998e0f405c433f9a16983aa2dfbcd3987/simpletransformers-0.18.11-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from simpletransformers) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from simpletransformers) (0.19.1)\n",
      "Requirement already satisfied, skipping upgrade: regex in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from simpletransformers) (2019.12.20)\n",
      "Collecting wandb\n",
      "  Using cached https://files.pythonhosted.org/packages/42/90/01aa4391f9fb3ec00c1aaa0928b101c0a3a2ec337adac5000aa3c8983e15/wandb-0.8.21-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pandas in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from simpletransformers) (0.25.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboardx in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from simpletransformers) (1.9)\n",
      "Processing /Users/liling.tan/Library/Caches/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68/seqeval-0.0.12-cp36-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from simpletransformers) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from simpletransformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from simpletransformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: transformers in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from simpletransformers) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from wandb->simpletransformers) (1.13.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Processing /Users/liling.tan/Library/Caches/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576/shortuuid-0.5.0-cp36-none-any.whl\n",
      "Processing /Users/liling.tan/Library/Caches/pip/wheels/e4/1d/06/640c93f5270d67d0247f30be91f232700d19023f9e66d735c7/nvidia_ml_py3-7.352.0-cp36-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: Click>=7.0 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from wandb->simpletransformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from wandb->simpletransformers) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/GitPython-2.1.11-py3.6.egg (from wandb->simpletransformers) (2.1.11)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/23/5a/f1b0c63e40517b06bc21744a94013ca05de21de2687a59de889ea20a9ebd/sentry_sdk-0.14.1-py2.py3-none-any.whl\n",
      "Processing /Users/liling.tan/Library/Caches/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23/gql-0.2.0-cp36-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: watchdog>=0.8.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from wandb->simpletransformers) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from wandb->simpletransformers) (5.4.5)\n",
      "Processing /Users/liling.tan/Library/Caches/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1/subprocess32-3.5.4-cp36-none-any.whl\n",
      "Collecting configparser>=3.8.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from pandas->simpletransformers) (2019.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
      "Collecting Keras>=2.2.4\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests->simpletransformers) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests->simpletransformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from requests->simpletransformers) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from transformers->simpletransformers) (1.9.239)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from transformers->simpletransformers) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from transformers->simpletransformers) (0.1.83)\n",
      "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gitdb2-2.0.5-py3.6.egg (from GitPython>=1.0.0->wandb->simpletransformers) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from gql==0.2.0->wandb->simpletransformers) (2.2.1)\n",
      "Processing /Users/liling.tan/Library/Caches/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5/graphql_core-1.1-cp36-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from watchdog>=0.8.3->wandb->simpletransformers) (3.12)\n",
      "Requirement already satisfied, skipping upgrade: argh>=0.24.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from watchdog>=0.8.3->wandb->simpletransformers) (0.26.2)\n",
      "Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from watchdog>=0.8.3->wandb->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (41.4.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.9.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.239 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (1.12.239)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: joblib in /Users/liling.tan/Library/Python/3.6/lib/python/site-packages (from sacremoses->transformers->simpletransformers) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/smmap2-2.0.5-py3.6.egg (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb->simpletransformers) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.239->boto3->transformers->simpletransformers) (0.14)\n",
      "Installing collected packages: docker-pycreds, shortuuid, nvidia-ml-py3, sentry-sdk, graphql-core, gql, subprocess32, configparser, wandb, Keras, seqeval, simpletransformers\n",
      "\u001b[33m  WARNING: The scripts wanbd, wandb, wandb-docker-run and wb are installed in '/Users/liling.tan/Library/Python/3.6/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Keras-2.3.1 configparser-4.0.2 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 nvidia-ml-py3-7.352.0 sentry-sdk-0.14.1 seqeval-0.0.12 shortuuid-0.5.0 simpletransformers-0.18.11 subprocess32-3.5.4 wandb-0.8.21\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install -U --user simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_for_st = list(zip(toxic_data.texts, toxic_data.labels))\n",
    "\n",
    "train_df = pd.DataFrame(training_data_for_st, columns=['text', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Explanation', 'Why', 'the', 'edits', 'made', 'under', 'my', 'username', 'Hardcore', 'Metallica', 'Fan', 'were', 'reverted', '?', 'They', 'were', \"n't\", 'vandalisms', ',', 'just', 'closure', 'on', 'some', 'GAs', 'after', 'I', 'voted', 'at', 'New', 'York', 'Dolls', 'FAC', '.', 'And', 'please', 'do', \"n't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'I', \"'m\", 'retired', 'now.89.205.38.27'], tensor([0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(training_data_for_st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelClassificationModel('roberta', 'roberta-base', \n",
    "                                      num_labels=6, \n",
    "                                      args={'reprocess_input_data': True, \n",
    "                                            'overwrite_output_dir': True, \n",
    "                                            'num_train_epochs': 5},\n",
    "                                     use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    }
   ],
   "source": [
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
